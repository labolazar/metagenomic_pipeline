[["index.html", "Protocol for metagenomic analyses 1 Introduction", " Protocol for metagenomic analyses Karine Villeneuve 2024-08-20 1 Introduction This guide assumes the following : The user has access to servers from the Lazar lab. The user has an active VPN access. The paired-end fastq files from Illumina Miseq sequencing were transferred to the user’s home directory. The user has followed the Introduction to linux guide and is comfortable with basic command line functions such as : listing files inside a current directory (ls) ; moving from one directory to the other (cd) ; creating new directory (mkdir) ; and moving / copying (mv/cp) files from one directory to the other. 1. Setting up your environment Keeping your files organized is a skill that has a high long-term payoff. As you are in the thick of an analysis, you may underestimate how many files/folders you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped, which can ultimately lead to significantly slower research progress. Furthermore, one must keep in mind that someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. While there’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution, below we propose a simple organizational scheme which is project-oriented, maintainable and ultimately follows consistent patterns for metagenome sequence processing Please note that the proposed workflow assumes such organization. ┌─ ~ -------------------------------- Your home directory │ ├── chapter1_metagenomics_aquifer ------ Project with a short but meaningfull name │ ├── 01_raw_data -------------------- Raw files (i.e. fastq files generated by sequencing) │ ├── 02_preprocess ------------------ Intermediates files from the trimming and interleaving process │ ├── interleave │ └── fastqc │ └── single │ ├── 03_trim_interleave ------------- Trimmed and interleaved fastq and fasta files used for assembly │ └── spades_out │ ├── 04_contigs --------------------- Assembled contigs │ ├── 05_binning --------------------- Intermediates files from the binning process │ ├── metabat2 │ ├── sample_01 │ ├── sample_02 │ ├── sample_03 │ ├── sorted_bam │ ├── 06_mags ------------------------ Clean and complete metagenomes assembled genomes (MAGs) │ ├── 07_metabolic_pathway ----------- metabolic_pathways │ └── 08_phylo_tree ------------------ Project-specific scripts └────────────────────────────────────────────────────────────────── Further reading about organizing files and folders : Organizing your project by the Johns Hopkins Data Science Lab A Quick Guide to Organizing Computational Biology Projects by William Stafford Noble, 2009 Reddit post Organizing your data by The Max Delbrück Center 2. Download fastqs files from Illumina BaseSpace Sequence Hub Open the link found in the email sent by the sequencing center. If this is your first time downloading your fastqs create a new BaseSpace Sequence Hub account using the email address to which the email from the sequencing center was addressed (normally this would be your UQAM’s email) On the pop-up window informing you that the sequencing center has shared the following item with you click ACCEPT. Click on the PROJECTS tab in the upper section of the page. Select your project and then click on the second round logo from the left which looks like a blank page and in the drop-down menu select DOWNLOAD then PROJECT. If required, download the Illumina Basespace downloader by clicking INSTALL DOWNLOAD and follow the instructions. Otherwise simply click DOWNLOAD to begin downloading your fastqs. Once the download is complete you will find inside the folder a folder for each of your sample inside which the forward and reverse read are both found in another folder. Instead of going into each folder individually and copying the fastqs manually we can use the terminal to do the job for us. From a new local terminal window navigate to the folder containing all the folders and execute the following command after having modified /path/to/directory/where/to/move/fastqs to the actual path where you wish to move your fastqs. find ./ -name &quot;*.gz&quot; -exec cp -prv &quot;{}&quot; &quot;/path/to/directory/where/to/move/fastqs&quot; &quot;;&quot; Finally you can transfer your fastqs to the folder 01_raw_data under your folder on the server using any File Transfer Protocol (FTP) clients (such as FileZilla or Cyberduck) or using the SCP (secure copy) command-line utility. For SCP you can copy an entire folder by opening a new local terminal window and navigating to the directory containing the folder with the fastqs. From that directory execute the following command. You will then be asked to enter the password for your user on the server. scp -r name_of_foler_with_fastqs username@server.bio.uqam.ca:/path/to/copy/folder Most process take all lot of time complete and therefore nohup should be used to execute the commands. For more details on how to use and examples see section Nohup. "],["pre-process.html", "2 Pre-process", " 2 Pre-process 1. Unzip Copy all your raw fastq.gz files into the folder called 02_preprocess. Move into the folder and unzip the files. gunzip *.gz 2. Interleave We are using a modified version of the script interleave.py from this GitHub gist to interleave the forward and reverse read. Create a bash script called run_interleave.sh with the following commands and execute the script using nohup. #!/bin/bash for R1 in *R1*.fastq; do python3 /home/SCRIPT/interleave.py $R1 &quot;${R1/R1/R2}&quot; &gt; $R1.interleave.fastq ; done Output : For each sample given as input (not file) the script generates a new file ending in .interleave.fastq. Create a new directory called interleave and move all the .interleave.fastq in this new directory. Move into this new directory for downstream analysis. View the top of the new interleaved files to make sure your reads alternate between R1 and R2. Replace sample-name with the name of the sample you want to verify. For sample sequenced on the NovaSeq replace @M for @A. grep @M sample-name.interleave.fastq | head 3. Trim Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3’-end of reads and also determines when the quality is sufficiently high enough to trim the 5’-end of reads. Create a bash script called run_sickle.sh with the following commands and execute the script using nohup. #!/bin/bash for i in *.interleave.fastq do sickle pe -c $i -t sanger -m $i.trim.fastq -s $i.singles.fastq done sickle pe (paired end) -c (inputfile) -t sanger (from illumina) -m (outputfilename) -s (exclutedreadsfilename) Output : For each given file Sickle generates two files (one ending in .interleave.fastq.trim.fastq and the other ending in .interleave.fastq.singles.fastq). For downstream processing we only need the .interleave.fastq.trim.fastq and therefore you can move all the .single into a new directory called single. 4. Quality check FastQc provides a modular set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. Create a new folder directory called fastqc which is where the HTLM output of FastQc will be saved. Create a bash script called run_fastqc.sh with the following commands and execute the script using nohup. #!/bin/bash fastqc *.interleave.fastq.trim.fastq --outdir=fastqc Output : For each given file FastQc generates an HTML file. The quality of the samples can be assessed by transferring the HTML files to your local computer. To determine if trimming caused any issue you can also run FastQc on the unzipped raw file and compare the results with the interleaved-trimmed file. 5. Transfer to Fasta Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Move all the .interleave.fastq.trim.fastq to the folder 03_trim_interleave and move into this folder. Create a bash script called run_seqtk.sh with the following commands and execute the script using nohup. #!/bin/bash gzip *.fastq for i in *.gz ; do seqtk seq -a $i &gt; $i.fasta ; done "],["assembly-spades.html", "3 Assembly SPAdes", " 3 Assembly SPAdes 1. Running SPAdes SPAdes - St. Petersburg genome assembler - is a versatile toolkit designed for assembling and analyzing sequencing data from Illumina and IonTorrent technologies. SPAdes package provides pipelines for DNA assembly of isolates and single-cell bacteria, as well as of metagenomic and transcriptomic data. Before running SPAdes you need to add the location of the program to your path with the following command : export PATH=/home/SCRIPT/SPAdes-3.15.5-Linux/bin:$PATH SPAdes takes as input paired-end reads, mate-pairs and single (unpaired) reads in FASTA and FASTQ (can be gzipped) formats. From the directory 03_trim_interleave create a bash script called run_spades.sh with the following commands and execute the script using nohup. Before running update the path to reflect the actual path where you want to output the results from SPAdes. #!/bin/bash for i in *.fastq do metaspades.py --12 $i -o /home/genomics/user/03_trim_interleave/spades_out/$i -t 40 done For each file given to SPAdes, the program will generate in the specified directory (here spades_out) a new folder with the full sample name (which should be quite long at this point) and inside each folder the assembled fasta file (called contigs.fasta). The following section allows the user to rename each folder created by SPAdes to a shorter name and then paste this name in front of each contigs.fasta file. 2. Rename SPAdes folder output Create a csv file called rename_folders.csv. In the file for each folder write the old file name followed by the new file name separated by a comma. Example : old_file_name,new_file_name sample_1_R1.fastq.interleave.fastq.trim.fastq,sample_01 sample_2_R1.fastq.interleave.fastq.trim.fastq,sample_02 sample_3_R1.fastq.interleave.fastq.trim.fastq,sample_03 sample_10_R1.fastq.interleave.fastq.trim.fastq,sample_10 sample_11_R1.fastq.interleave.fastq.trim.fastq,sample_11 Convert csv to unix format using dos2unix : dos2unix rename_folders.csv Rename folders using awk : awk -F&#39;,&#39; &#39;system(&quot;mv &quot; $1 &quot; &quot; $2)&#39; rename_folders.csv From the directory containing all the renamed folders (here spades_out) use the following for loop to (1) go through every folder and find a file called contigs.fasta ; (2) add to the beginning of this file name it’s current directory name ; (3) move the file one folder up : for folder in *; do (cd &quot;$folder&quot; &amp;&amp; contigs.fasta) mv &quot;$folder/contigs.fasta&quot; &quot;${folder}_contigs.fasta&quot; done Move all the _contigs.fasta file to the directory 04_contigs and move into this directory. 3. Post assembly statistics To count how many contigs were generated we use grep to count (-c) the occurrences of the symbol &gt; (every contig starts with this symbol) : grep -c &quot;&gt;&quot; *.fasta Use BBMap to filter out short contigs (&lt; 1000 base pairs). for file in *.fasta ; do ~/bbmap/reformat.sh in=$file out=1kbp_$file minlength=1000 ; done Use grep once again to count how many contigs passed filtering. For downstream analysis your samples should have at least 5000 base pairs. 4. Other assemblers These are other assemblers that were useful in the past but that we do not use anymore as SPAdes always the best results. IDBA. The output is a directory ending in assembly for each sample. In this directory you will find the contig file. For DNA #!/bin/bash for i in *.fasta do idba_ud -l -r $i -o /home/genomics/user/03_trim_interleave/idba/$i --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 40 done For RNA #!/bin/bash for i in *.fasta do idba_tran -l -r $i -o /home/genomics/user/03_trim_interleave/idba/$i --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 40 done Megahit #!/bin/bash for i in *.fasta do megahit --12 $i --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 40 -o /home/genomics/user/03_trim_interleave/Megahit/$i --out-prefix megahit_$i done More ressources : About co-assembly Cross-Assembly pipeline Further reading "],["barrnap.html", "4 Barrnap", " 4 Barrnap 1. Barrnap Barrnap (BAsic Rapid Ribosomal RNA Predictor) predicts the location of ribosomal RNA genes in genomes. It supports bacteria (5S,23S,16S), archaea (5S,5.8S,23S,16S), metazoan mitochondria (12S,16S) and eukaryotes (5S,5.8S,28S,18S). Barrnap takes FASTA DNA sequence as input and can be used on both the assembled contigs (community) (directory 04_contigs) and MAGs (directory 06_mags). Add the name of the sample at the beginning of every scaffhold ID and change the extension type to .fna (same as .fasta, we are simply using a different extension to distinguish new file with the sample name in front of each scaffhold from the old file). for i in *.fasta ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done Change the space between the name and the scaffhold number to an underscore sed -i &#39;s/ /_/g&#39; *.fna Run Barrnap. Create a bash script called run_barrnap.sh with the following commands and execute the script using nohup. #!/bin/bash for i in *.fna ; do barrnap $i &gt; $i.barrnap_hits.gff --threads 20 ; done Output : For each given file Barrnap generates a GFF file as output which includes the coordinates of where the rRNA genes are encoded. To extract the ribosomal sequences as FASTA we are using the function getfasta from bedtools. 2. Extract sequences The bedtools utilies are a swiss-army knife of tools for a wide-range of genomics analysis tasks. Here we use the getfasta command to extract sequences from a FASTA file for each of the intervals defined in the GFF3 file generated by Barrnap. #!/bin/bash for i in *.fna ; do bedtools getfasta -fi $i -bed $i.barrnap_hits.gff -fo $i.out_rRNA.fasta ; done Transfer the .out_rRNA.fasta to your local computer and use NCBI-BLAST to classify the rRNA sequences identified by Barrnap. "],["binning.html", "5 Binning", " 5 Binning In metagenomics, binning is the process of grouping reads or contigs and assigning them to individual genome known as Metagenome Assembled Genome (MAG). Coverage-based binning approaches will require you to map reads to assembled contigs. Comparision of different binning tools 1. Mapping Mapping allows you to get a rough count of the total number of reads mapping to each contig, also referred to as the depth file or read coverage. This information is required for most binning algorithm. Raw reads from each sample are mapped to the contigs (assembled reads).The idea being that if two contigs come from the same genome in your sample, so the same organism, then they would have been sequenced roughly to the same depth, they would have a similar coverage and they go together. It gets even better if you have multiple samples that vary a bit and you sequence all of them. You do your assembly on one of the samples, then you can take the reads from the other samples, map to that genome (metagenomic assembly) and you can use that as additional information. Different tools exists for mapping reads to genomic sequences. Here we are using Scons Wrapper which maps FASTQ reads against an assembly (e.g. genome) in FASTA format using BWA-MEM. This wrapper does not produce huge intermediate files (e.g. unfiltered SAM). I am currently looking for a way to loop this. Tried specifying only the input folder and the sampleids as proposed in the README but it does not work. Therefore these steps must be repeated for every sample. . For each sample, create a new folder in the directory 05_binning and inside each folder copy the filtered assembled contigs FASTA and the interleaved-trimed FASTQ for that sample. Activate the scons_maps conda environment conda activate scons_map Execute scons from the genome_mapping directory (/home/genomics/genome_mapping). Modify the following parameters accordingly : --fastq_dir = directory where the filtered assembled contigs FASTA and the interleaved-trimed FASTQ files are. --assembly = path to the filtered contig FASTA files including the name of the file at the end. --sampleids = name of the interleave.trim.fastq. scons --fastq_dir=/home/genomics/user/05_binning/sample_01/ --assembly=/home/genomics/user/05_binning/sample_01/1kb_sample_01_contigs.fasta --outdir=/home/genomics/user/05_binning/sample_01/output --sampleids=sample_01.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=0 --tmpdir=/home/genomics/tmp --logfile=mapping.log In the specified output directory you will find a .sorted.bam which contains the depth information required by MetaBAT2. Create a directory call sorted_bam and move all the sorted.bam files into this directory. 2. Depth file The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use. We use the script jgi_summarize_bam_contig_depths. From the directory sorted_bam create a bash script called run_depthfile.sh with the following commands and execute the script using nohup. #!/bin/bash for i in *.sorted.bam do /home/SCRIPT/jgi_summarize_bam_contig_depths --outputDepth $i.depth.txt --pairedContigs $i.paired.txt $i done Output : For each given file the script generates a file ending with .depth.txt as output. Move these files into a directory call metabat2 and also copy into this folder the filtered contig FASTA. 3. Create bins MetaBAT2 is an efficient tool for accurately reconstructing single genomes from complex microbial communities. If required, first deactivate scons_map and then activate conda. conda deactivate conda activate In order to be able to loop this for throught all samples, the name of filtered contig FASTA file must match exactly the beginning of the depths file. Example : Filtered contig FASTA : 1kb_sample_01_contigs.fasta Depth file : 1kb_sample_01_contigs.fasta.depth.txt Create a bash script called run_metabat2.sh with the following commands and execute the script using nohup. #!/bin/bash for i in *.fasta do metabat2 -i $i -a $i.depth.txt -o bins_$i -t 0 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 1000 done Output - for each given file metabat2 generates a directory containing all the bins created. 4. Check quality Checkm You have to go back one folder in the terminal as checkm will run on all the files in the folder you give it as input. Checkm will automatically create a folder called checkm in the specified directory, therefor if you must run checkm again make sure to delete the newly created checkm folder, otherwise checkm will give you an error message. Activate the environment conda activate checkm Create a bash script called run_checkm.sh with the following commands and execute the script using nohup. checkm lineage_wf -x fa 2kbp_bins/ 2kbp_bins/checkm -f 2kbp_bins/output.txt -t 48 Open the output.txt document with excel to verify the completeness and contamination of your bins. Standard : Completeness &gt; 50 % and Contamination &lt; 10 % Remove all the spaces with control + H Filter the columns by Completeness, and separate the ones &lt; 50 % by adding a line in excel Filter by Contamination, and highlight all the ones &gt; 10 % - These are the bins you want to clean 5. Metagenome assembled genome statistics Quast #!/bin/bash for i in *.fa do python /home/genomics/quast-5.2.0/quast.py $i -o /home/genomics/karine/stleonard/E4-2/quast_out/$i --threads 48 --no-check --no-plots --no-html --no-icarus done for folder in *; do (cd &quot;$folder&quot; &amp;&amp; report.txt) mv &quot;$folder/report.txt&quot; &quot;${folder}_report.txt&quot; done 6. Taxonomy GTDBTK Activate the GTDBTK environment conda activate gtdbtk-2.1.1 I located the folder with the untar GTDBTK data (GTDBTk_data/release214) and I added the path to this file to my ~/.profile (using vi) export GTDBTK_DATA_PATH=/home/genomics/release214 In the folder with all your clean and completed genomes run this command with nohup #!/bin/bash gtdbtk classify_wf --cpus 20 --genome_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp --out_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp/gtdbk_output -x fa Once it is done running, you can open the folder called gtdbk_output and copy the folder gtdbtk.bac120.summary.tsv to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. "],["metabolic-pathway.html", "6 Metabolic pathway", " 6 Metabolic pathway 1. BlastKOALA BlastKOALA is an automatic annotation servers for genome and metagenome sequences, which perform KO (KEGG Orthology) assignments to characterize individual gene functions and reconstruct KEGG pathways, BRITE hierarchies and KEGG modules to infer high-level functions of the organism or the ecosystem. BlastKOALA takes as input amino acid sequences in FASTA format. We therefore use prodigal to translate nucleic acid sequences to the corresponding peptide sequences.By default prodigal takes as input FASTA files with the .fna extension. Use perl to add the name of the sample to the beginning of every file and change the file type to .fna. for i in *.fa ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done Replace space between the name of you bins and the scaffhold to underscores for i in *.fna ; do sed -i &#39;s/ /_/g&#39; $i ; done Run prodigal for i in *.fna ; do prodigal -i $i -o output.txt -a $i.faa ; done Remove all the characters after the first space in the header for i in *.faa; do sed -i &#39;s/\\s.*$//&#39; $i; done Transfer the files to your local computer and follow the instructions on the BlastKOALA website for submission. 2. Other programs we aren’t using anymore DRAM DRAM (Distilled and Refined Annotation of Metabolism) is a tool for annotating metagenomic assembled genomes and VirSorter identified viral contigs. DRAM annotates MAGs and viral contigs using KEGG (if provided by the user), UniRef90, PFAM, dbCAN, RefSeq viral, VOGDB and the MEROPS peptidase database as well as custom user databases. Activate environment conda activate DRAM Execute DRAM. Create a bash script called run_dram.sh with the following commands and execute the script using nohup.Note : DRAM requires the full path (home/genomics/…) and won’t accept path starting from the home directory (~). DRAM.py annotate -i &#39;/home/genomics/yourname/samples/*.fa&#39; -o /home/genomics/yourname/samples/annotation --threads 40 Once annotation is done, the following command will summarize all the results from the folder annotate DRAM.py distill -i annotation/annotations.tsv -o summaries --trna_path annotation/trnas.tsv --rrna_path annotation/rrnas.tsv Metabolic METABOLIC (METabolic And BiogeOchemistry anaLyses In miCrobes) enables the prediction of metabolic and biogeochemical functional trait profiles to any given genome datasets. Activate environment conda activate metabolic METABOLIC takes as input amino acid sequences in FASTA format. Create a bash script called run_metabolic.sh with the following commands and execute the script using nohup. Before running update the path to reflect the actual path where your amino acid FASTA files are located and where you want METABOLIC to store the output files. #!/bin/bash perl /home/genomics/METABOLIC/METABOLIC-G.pl -in /home/genomics/user/07_metabolic_pathway/ -o /home/genomics/user/07_metabolic_pathway/metabolic_outut/ MEBS [MEBS] (https://github.com/valdeanda/mebs)(Multigenomic Entropy-Based Score) allows the user to synthesizes genomic information into a single informative value. This entropy score can be used to infer the likelihood that microbial taxa perform specific metabolic-biogeochemical pathways. For the moment MEBS shoudl be installed on the user’s local computer. See manual for installation and instructions. "],["phylogenetic-tree.html", "7 Phylogenetic tree", " 7 Phylogenetic tree 1. Downloading reference genomes Download the latest NCBI assembly_summary_genbank.txt file using wget. Note that the link might change over time. wget https://ftp.ncbi.nih.gov/genomes/genbank/assembly_summary_genbank.txt For each phylum, order, class, family of interest, search in the NCBI database and select 5 individuals with the most complete genome. Copy/paste the name and the assembly number in an excel document (seperate columns - Name and Assembly_GCA). From that excel document, copy the list of assembly number including the column name (Assembly_GCA) in a vi document called GCA.txt. Search for those identifiers in the assembly summary genbank file with the following command and export the columns (1,7,8,20*) we are interested in to a tabular file called todownload.tab. for n in `cat GCA.txt`; do grep $n assembly_summary_genbank.txt | cut -f 1,7,8,16,20 &gt;&gt; todownload.tab ; done Column 1: “assembly_accession” - Column 7: “species_taxid” - Column 8: “organism_name” - Column 16: “asm_name” - Column 20: “ftp_path” Run the Rscript getFTPlink.r which generates two files 1. ftp.links.txt : contains the ftp link to the genomic.fna.gz file for each genomes of interest. Download the files using wget and unzip them. 2. name.txt : contains two column, first is the name of the fasta file followed by organsim name and asm name. This file will be used to change the branch names for the tree Rscript /home/SCRIPT/getFTPlink.r wget -i ftp.links.txt gunzip *.gz Create a new directory and move the ref files there and run CheckM on the downloaded genomes. Discard the genomes that are not completed. conda activate checkm checkm lineage_wf -x fna ref_genomes checkm -f output_table.txt -t 48 Move your bins and reference genomes in a folder together. Add the name of the bin to the beginning of every file and change the file type to .fna. for i in *.fa ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done 2. GToTree GToTree Move the file name.txt into the folder containing your genomes. Add to this file the name of your bins. Create a txt file called fasta_files.txt containing the name of all the fasta files you have. ls *.fna &gt; fasta_files.txt conda activate gtotree GToTree -f fasta_files.txt -H Bacteria -j 4 -o out -G 0 Upload the tree to itol "],["viromes.html", "8 Viromes", " 8 Viromes Useful links from the JGI workshop : Get Started with KBase Viral Genomics Webinar Q &amp; A "],["using-nohup-and-other-tips-and-tricks.html", "9 Using nohup and other tips and tricks 9.1 Bash scripting using nohup", " 9 Using nohup and other tips and tricks 9.1 Bash scripting using nohup Fist step is to write your script into a text editor (vi or nano). I suggest using vi as it is a very powerful text editor with plenty of useful shortcuts. You can find more information on vi here. To create a new file the format of the command is as follow : vi script.sh First we call the program vi followed by the name of file to create. The extension of the file needs to reflect the type of file you want to create (i.e. for a python script the extension would be .py, a bash script would be .sh, and a simple text file would be .txt. Since the commands we are writing are for executed by bash we need to use the extension .sh. In the new opened window write your script. To start typing with vi press the i touch. Once you are done exit vi by pressing esc then : followed by w and q. If you want to exit without saving your changes press q and ! instead. Make your script executable. If you look at the file using ll the script should now be green and followed by *. chmod +x script.sh Run using nohup nohup ./script.sh &amp; After pressing nohup the prompt will print your process ID in the following format [1] xxxxxxxx and print the following : nohup: ignoring input and appending output to 'nohup.out' Press enter again to return to the prompt. Your process is completed when you get the following [1]+ Done nohup ./script.sh. If nohup exits there was an error running the script. To view the output and the find out the error, view the report using less nohup.out. Exit less with q View jobs status jobs Once nohup is done running your script it will write Done "],["other-tips-and-tricks.html", "10 Other tips and tricks 10.1 Moving files using SCP 10.2 Renaiming files 10.3 Extract scaffhold based on nane and coordinates using samtools", " 10 Other tips and tricks To move or copy many files listed in a text files xargs -a file_list.txt mv -t /path/to/directory 10.1 Moving files using SCP To move file from your computer to the server, from your local terminal scp filename.txt servername@server.bio.uqam.ca:/home/user/pathway/todirectory To move file from your the server, to your computer scp kvilleneuve@titan.bio.uqam.ca:/home/kvilleneuve/filename.txt . 10.2 Renaiming files Replace the pattern in brackets for the section of words you want to remove from the name of your file. for i in *.fastq; do mv $i &quot;$(echo $i | sed s/&quot;_R1.fastq.interleave.fastq.trim.fastq.gz.&quot;/./)&quot;; done 10.3 Extract scaffhold based on nane and coordinates using samtools Copy the scaffhold name followed by the coordinates in a vi file. Example : contig-115_1168:1-955 Extract the sequence identified in the vi file into a new file using samtools for i in `cat scaff`; do samtools faidx 2000kb.fa $i &gt;&gt;16S_2Kb.fna; done "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
