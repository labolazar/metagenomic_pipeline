[["index.html", "Protocol for metagenomic analyses 1 Introduction 1.1 Setting up your environment 1.2 Download fastqs files from Illumina BaseSpace Sequence Hub", " Protocol for metagenomic analyses Karine Villeneuve 2024-08-15 1 Introduction This guide assumes the following : The user has access to servers from the Lazar lab. The user has an active VPN access. The paired-end fastq files from Illumina Miseq sequencing were transferred to the user’s home directory. The user has followed the Introduction to linux guide and is comfortable with basic command line functions such as : listing files inside a current directory (ls) ; moving from one directory to the other (cd) ; creating new directory (mkdir) ; and moving / copying (mv/cp) files from one directory to the other. 1.1 Setting up your environment Keeping your files organized is a skill that has a high long-term payoff. As you are in the thick of an analysis, you may underestimate how many files/folders you have floating around. But a short time later, you may return to your files and realize your organization was not as clear as you hoped, which can ultimately lead to significantly slower research progress. Furthermore, one must keep in mind that someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. While there’s a lot of ways to keep your files organized, and there’s not a “one size fits all” organizational solution, below we propose a simple organizational scheme which is project-oriented, maintainable and ultimately follows consistent patterns for amplicon sequence analysis. Please note that the proposed workflow assumes such organization. ┌─ ~ -------------------------------- Your home directory │ ├── chapter1_metagenomics_aquifer ------ Project with a short but meaningfull name │ ├── 01_raw_data -------------------- Raw files (i.e. fastq files generated by sequencing) │ ├── 02_preprocess ------------------ Intermediates files from the trimming and interleaving process │ ├── 03_trim_interleave ------------- Trimmed and interleaved fastq and fasta files used for assembly │ ├── 04_contigs --------------------- │ ├── 05_ ------- ------------------ Project-specific final datasets for publication │ ├── 03_assembly │ ├── figures ----------------- Project-specific final figures for publication │ └── scripts ----------------- Project-specific scripts └────────────────────────────────────────────────────────────────── Further reading about organizing files and folders : Organizing your project by the Johns Hopkins Data Science Lab A Quick Guide to Organizing Computational Biology Projects by William Stafford Noble, 2009 Reddit post Organizing your data by The Max Delbrück Center 1.2 Download fastqs files from Illumina BaseSpace Sequence Hub Open the link found in the email sent by the sequencing center. If this is your first time downloading your fastqs create a new BaseSpace Sequence Hub account using the email address to which the email from the sequencing center was addressed (normally this would be your UQAM’s email) On the pop-up window informing you that the sequencing center has shared the following item with you click ACCEPT. Click on the PROJECTS tab in the upper section of the page. Select your project and then click on the second round logo from the left which looks like a blank page and in the drop-down menu select DOWNLOAD then PROJECT. If required, download the Illumina Basespace downloader by clicking INSTALL DOWNLOAD and follow the instructions. Otherwise simply click DOWNLOAD to begin downloading your fastqs. Once the download is complete you will find inside the folder a folder for each of your sample inside which the forward and reverse read are both found in another folder. Instead of going into each folder individually and copying the fastqs manually we can use the terminal to do the job for us. From a new local terminal window navigate to the folder containing all the folders and execute the following command after having modified /path/to/directory/where/to/move/fastqs to the actual path where you wish to move your fastqs. find ./ -name &quot;*.gz&quot; -exec cp -prv &quot;{}&quot; &quot;/path/to/directory/where/to/move/fastqs&quot; &quot;;&quot; Finally you can transfer your fastqs to the folder 01_raw_data under your folder on the server using any File Transfer Protocol (FTP) clients (such as FileZilla or Cyberduck) or using the SCP (secure copy) command-line utility. For SCP you can copy an entire folder by opening a new local terminal window and navigating to the directory containing the folder with the fastqs. From that directory execute the following command. You will then be asked to enter the password for your user on the server. scp -r name_of_foler_with_fastqs username@server.bio.uqam.ca:/path/to/copy/folder Some program may take some time to process, we therefor recommend running the command using a shell script with nohup. For example on how to use, see section Nohup. Furthermore, two ways of running most command are described; (1) for one or two samples and (2) for multiple samples using Bash For Loop. Conda and all the required environment have all been previously installed by Karine Villeneuve. "],["pre-assembly.html", "2 Pre-Assembly", " 2 Pre-Assembly Unzip Copy all your raw fastq.gz files into a folder called 02_preprocess. Move into the folder and unzip the files. gunzip *.gz Interleave We are using a modified version of the script interleave.py from this GitHub gist to interleave the forward and reverse read. Create bash script called run_interleave.sh with the following commands and execute the script using nohup. #!/bin/bash for R1 in *R1*.fastq; do python3 /home/SCRIPT/interleave.py $R1 &quot;${R1/R1/R2}&quot; &gt; $R1.interleave.fastq ; done Output : For each given sample (not file) as input it generates a new file ending in .interleave.fastq will be generated. Create a new directory called interleave and move all the .interleave.fastq in this new directory. Move into this new directory or downstream analysis. View the top of the new interleaved files to make sure your reads alternate between R1 and R2. Replace sample-name with the name of the sample you want to verify. For sample sequenced on the NovaSeq replace @M for @A. grep @M sample-name.interleave.fastq | head Trim Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3’-end of reads and also determines when the quality is sufficiently high enough to trim the 5’-end of reads. Create bash script called run_sickle.sh with the following commands and execute the script using nohup. #!/bin/bash for i in *.interleave.fastq do sickle pe -c $i -t sanger -m $i.trim.fastq -s $i.singles.fastq done sickle pe (paired end) -c (inputfile) -t sanger (from illumina) -m (outputfilename) -s (exclutedreadsfilename) Output : For each given file it generates two files (one ending in .interleave.fastq.trim.fastq and the other ending in .interleave.fastq.singles.fastq). For downstream processing we only need the .interleave.fastq.trim.fastq and therefore you can move all the .single into a new directory called single. Quality check FastQc provides a modular set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. Create a new folder directory called fastqc which is where the HTLM output of FastQc will be saved. Create bash script called run_fastqc.sh with the following commands and execute the script using nohup. #!/bin/bash fastqc *.interleave.fastq.trim.fastq --outdir=fastqc Output : For each given file it generates an HTML file. The quality of the samples can be assessed by transferring the HTML files to your local computer. To determine if trimming caused any issue you can also run FastQc on the unzipped raw file and compare the results with the interleaved-trimmed file. Transfer to Fasta Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Move all the .interleave.fastq.trim.fastq to a new folder called 03_trim_interleave #!/bin/bash gzip *.fastq for i in *.gz ; do seqtk seq -a $i &gt; $i.fasta ; done "],["assembly.html", "3 Assembly 3.1 Assemblers 3.2 Post assembly-stat", " 3 Assembly 3.1 Assemblers For the moment, there is no way of knowing which assembly program is best suited for your sample. Therefor, I recommend trying different assemblers and looking at (1) the total number of contigs, (2) the number of contigs &gt; 2000 bp and (3) the number of contigs &gt; 1000 bp. Because the assembly process is quite long I recommend always using nohup. Most assembler will produce a file called contig.fa or contigs.fa which is the file we will be using to generate bins. About co-assembly Cross-Assembly pipeline Further reading From the same directory containing 01_Pre-Assembly, create a new directory called 02_assembly. In this new directory create a directory for every assembly program (IDBA, Megahit, Metaspades) #!/bin/bash mkdir 02_assembly cd 02_assembly mkdir IDBA mkdir Megahit mkdir Metaspades cd .. 3.1.1 IDBA Github. The output is a directory ending in assembly for each sample. In this directory you will find the contig file. Run IDBA using nohup For DNA #!/bin/bash for i in *.fasta do idba_ud -l -r $i -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/IDBA/$i --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 40 done For RNA #!/bin/bash for i in *.fasta do idba_tran -l -r $i -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/IDBA/$i --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 40 done 3.1.2 Megahit Github Run Megahit using nohup #!/bin/bash for i in *.fasta do megahit --12 $i --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 40 -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/Megahit/$i --out-prefix megahit_$i done 3.1.3 Metaspades Github. We are using the fastq files. To run Metaspades you will need to add the location of the program to your path using the following command export PATH=/home/SCRIPT/SPAdes-3.15.5-Linux/bin:$PATH Run Metaspades using nohup #!/bin/bash for i in *.fastq do metaspades.py --12 $i -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/Metaspades/$i -t 40 done 3.2 Post assembly-stat To determine which assembly method gave you the best results you need to look at the output of every sample individually. For every sample, the assembler will generate a folder containing a file called contig.fa. To know how many contigs were generated we use this command which counts the number of “&gt;” (every contig start with this symbol): grep -c &quot;&gt;&quot; contig.fa We also want to know how many contig have more than 2000 base pairs (2KB) and 1000 base pairs (1KB). For each of your sample, run the following script which will output two files : - 2kb.fasta (the number of contigs with length greater than 2000 basepair) - 1kb.fasta (the number of contigs with length greater than 1000 basepair) bash /home/SCRIPT/countBP.sh #!/bin/bash perl -lne &#39;if(/^(&gt;.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print &quot;$s\\n$fa{$s}\\n&quot; if(length($fa{$s})&gt;1000) }}&#39; contig.fa &gt; 1kb.fa perl -lne &#39;if(/^(&gt;.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print &quot;$s\\n$fa{$s}\\n&quot; if(length($fa{$s})&gt;2000) }}&#39; contig.fa &gt; 2kb.fa grep -c &quot;&gt;&quot; 1kb.fa grep -c &quot;&gt;&quot; 2kb.fa "],["binning.html", "4 Binning 4.1 Mapping 4.2 Depth file 4.3 Create bins", " 4 Binning In metagenomics, binning is the process of grouping reads or contigs and assigning them to individual genome known as Metagenome Assembled Genome (MAG). Coverage-based binning approaches will require you to map reads to assembled contigs. Comparision of different binning tools 4.1 Mapping Mapping allows you to get a rough count of the total number of reads mapping to each contig, also referred to as the depth file or read coverage. This information is required for most binning algorithm. Raw reads from each sample are mapped to the contigs (assembled reads).The idea being that if two contigs come from the same genome in your sample, so the same organism, then they would have been sequenced roughly to the same depth, they would have a similar coverage and they go together. It gets even better if you have multiple samples that vary a bit and you sequence all of them. You do your assembly on one of the samples, then you can take the reads from the other samples, map to that genome (metagenomic assembly) and you can use that as additional information. Start by creating a new directory with all your assembled contigs and your interleave.trim.fastq. Different tools exists for mapping reads to genomic sequences. Here we are using Scons wrapper. 4.1.1 Scons Wrapper (genome mapping) Github This wrapper maps FASTQ reads against an assembly (e.g. genome) in FASTA format using BWA-MEM. This wrapper does not produce huge intermediate files (e.g. unfiltered SAM). For each sample, create a folder and copy into this folder these two files : the long contig.fa (1000 or 2000 bp) and the trim and interleaved fastq (fastq after sickle) Go to your home directory and clone the git repository git clone https://github.com/imrambo/genome_mapping.git Go to the new directory created genome_mapping and activate the scons_maps conda environment conda activate scons_map Run a dry run to ensure everything runs smoothly (you must run this from the genome_mapping directory) scons --dry-run --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log Run the script from the genome_mapping directory. –fastq_dir=directory where the interleave_trim_fastq and assembled_long_contig_fasta files are. –assembly=path to the assembled_long_contig_fasta files. Must include the name of the file at the end. –sampleids=the name of the interleave_trim_fastq. scons --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log In the specified output directory you will find a .sorted.bam which contains the depth information required for mmgenome and metabat. Currently looking for a way to loop this. Tried specifying only the input folder and the sampleids as proposed in the README but it does not work. sampleids must be the exact name as the fastq or fastq.gz file. 4.2 Depth file The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use. We use the script jgi_summarize_bam_contig_depths. Move all the sorted.bam files into a new folder called Sorted_Bam and from this folder use nohup to run the script jgi_summarize_bam_contig_depths. #!/bin/bash for i in *.sorted.bam do /home/SCRIPT/jgi_summarize_bam_contig_depths --outputDepth $i.depth.txt --pairedContigs $i.paired.txt $i done 4.3 Create bins 4.3.1 MetaBAT2 Github / Article Efficient tool for accurately reconstructing single genomes from complex microbial communities MetaBAT2 requires that your python environment be activate (base). If required, first deactivate scons_map and then activate conda conda deactivate conda activate In order to be able to loop this for all my samples, I renamed each depth file in the following format : sample1.fa.depth.txt. The output is a folder called bins_dir containing all the bins created. I recommend using nohup as the binning process can be very long. **copy all your 1kb or 2 kb fasta into the folder containing the depth file. Multiple samples #!/bin/bash for i in *.fa do metabat2 -i $i -a $i.depth.txt -o bins -t 0 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000 done minCVsum : assigning number of tetranucleotide frequency graphs, don’t grab negative numbers -m : min size of contig to be considered for binning "],["bin-quality.html", "5 Bin quality 5.1 Checkm 5.2 Installing Checkm", " 5 Bin quality 5.1 Checkm Github You have to go back one folder in the terminal as checkm will run on all the files in the folder you give it as input. Checkm will automatically create a folder called checkm in the specified directory, therefor if you must run checkm again make sure to delete the newly created checkm folder, otherwise checkm will give you an error message. You need to specify the extension of your file for it to work. For example, for file finishing is .fa the command will be checkm lineage_wf -x fa… If checkm is already installed on your system simply activate the environment conda activate checkm Run checkm using nohup checkm lineage_wf -x fa 2kbp_bins/ 2kbp_bins/checkm -f 2kbp_bins/output.txt -t 48 checkm lineage_wf -x fa 1500bp_bins 1500bp_bins/checkm -f 1500bp_bins/output.txt -t 48 –noAdd Open the output.txt document with excel to verify the completeness and contamination of your bins. Standard : Completeness &gt; 50 % and Contamination &lt; 10 % Remove all the spaces with control + H Filter the columns by Completeness, and separate the ones &lt; 50 % by adding a line in excel Filter by Contamination, and highlight all the ones &gt; 10 % - These are the bins you want to clean 5.2 Installing Checkm Install Checkm using the Installation through Conda steps. After installation is complete run the following to inform where the checkm databases are installed*: checkm data setRoot /usr/local/lib/checkm *I downloaded the checkm databases and moved them to /usr/local/lib/checkm. I decompressed the file using sudo tar -xf checkm_data_2015_01_16.tar.gz. I changed the File Ownership to root sudo chown -R root /usr/local/lib/checkm and Group Ownership to me sudo chgrp kvilleneuve /usr/local/lib/checkm. I ran the following to inform CheckM of where the files have been placed: checkm data setRoot /usr/local/lib/checkm "],["bin-cleaning.html", "6 Bin cleaning 6.1 Vizbin", " 6 Bin cleaning 6.1 Vizbin "],["taxonomy.html", "7 Taxonomy 7.1 GTDBTK 7.2 BAsic Rapid Ribosomal RNA Predictor (Barrnap)", " 7 Taxonomy 7.1 GTDBTK Github Activate the GTDBTK environment conda activate gtdbtk-2.1.1 I located the folder with the untar GTDBTK data (GTDBTk_data/release214) and I added the path to this file to my ~/.profile (using vi) export GTDBTK_DATA_PATH=/home/genomics/release214 In the folder with all your clean and completed genomes run this command with nohup #!/bin/bash gtdbtk classify_wf --cpus 20 --genome_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp --out_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp/gtdbk_output -x fa Once it is done running, you can open the folder called gtdbk_output and copy the folder gtdbtk.bac120.summary.tsv to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. 7.2 BAsic Rapid Ribosomal RNA Predictor (Barrnap) Github Barrnap predicts the location of ribosomal RNA genes in genomes. It supports bacteria (5S,23S,16S), archaea (5S,5.8S,23S,16S), metazoan mitochondria (12S,16S) and eukaryotes (5S,5.8S,28S,18S). You can run barrnap on both the assembled contigs (community) and MAGs. Add the name of the sample at the beginning of every contig and change the file type to .fna. Then for each of your file, change the space between the name and the scaffhold number to an underscore for i in *.fa ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done sed -i &#39;s/ /_/g&#39; *.fna Run barrnap barrnap 1kb.fasta.fna &gt; barrnap_hits.txt --threads 20 One very useful tool or parsing GFF files is called BEDtool (to install:sudo apt install bedtools). There are many different utilities in bedtools. Here we will want to use the “getfasta” option, which will allow us to supply the fasta file and the barrnap GFF file to obtain the rRNA sequences. Note that the GFF file has the coordinates of where the rRNA genes are encoded, so between the GFF file and the .FNA file we have all the information we need. bedtools getfasta -fi 1kb.fasta.fna -bed barrnap_hits.gff -fo out.rRNA.fasta Use NCBI-BLAST to classify the rRNA sequences identified by barrnap. "],["metabolic-pathway.html", "8 Metabolic pathway 8.1 DRAM 8.2 Metabolic 8.3 Mebs", " 8 Metabolic pathway Both DRAM and Metabolic are installed only for the user genomics - User : ssh genomics@titan.bio.uqam - Password : genomics Use sudo cp to copy the samples to annotate into your directory under the user genomics. 8.1 DRAM Github page Note : DRAM requires the full path (home/genomics/…) and won’t accept path starting from the home directory (~). conda activate DRAM DRAM.py annotate -i &#39;/home/genomics/yourname/samples/*.fa&#39; -o /home/genomics/yourname/samples/annotation --threads 40 Once annotation is done, the following command will summarize all the results from the folder annotate DRAM.py distill -i annotation/annotations.tsv -o summaries --trna_path annotation/trnas.tsv --rrna_path annotation/rrnas.tsv 8.1.1 Installing Because FTP connections weren’t allowed on the server, I changed the link to download the different databases from FTP to HTTPS. The file I modified is /home/kvilleneuve/anaconda/envs/DRAM/lib/python3.10/site-packages/mag_annotator/database_processing.py. 8.2 Metabolic Prodigal Github We use prodigal to translate nucleic acid sequences to the corresponding peptide sequences.By default prodigal takes as input fasta files with the .fna extension. Use perl to add the name of the sample to the beginning of every file and change the file type to .fna. for i in *.fa ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done Replace space between the name of you bins and the scaffhold to underscores for i in *.fna ; do sed -i &#39;s/ /_/g&#39; $i ; done Run prodigal to convert your bins to amino acid (faa) sequence for i in *.fna ; do prodigal -i $i -o output.txt -a $i.faa ; done Remove all the characters after the first space in the header for i in *.faa; do sed -i &#39;s/\\s.*$//&#39; $i; done Metabolic I am currently having issues again with running and installing metabolic…. Github conda activate metabolic Change the path to the directory containing the genome amino acid files ending with .faa Run using nohup #!/bin/bash perl /home/genomics/METABOLIC/METABOLIC-G.pl -in /path/to/faa/ -o output/ 8.2.1 Installing To install metabolic I copied the file metabolic_environment.txt file from the issue #27 on the server and renamed it metabolic_environment.yml I created the environment using conda env create -f metabolic_environment.yml and then followed all the other step described in the Github wiki There also seemed to be issues with the run_to_setup.bash. I solved this problem by changing these commands in the script : curl --silent ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz -O ko_list.gz curl --silent ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz -O profiles.tar.gz curl --silent ftp://ftp.ebi.ac.uk/pub/databases/merops/current_release/pepunit.lib -O pepunit.lib for these commands : curl https://www.genome.jp/ftp/db/kofam/ko_list.gz -output ko_list.gz curl https://www.genome.jp/ftp/db/kofam/profiles.tar.gz -output profiles.tar.gz curl https://ftp.ebi.ac.uk/pub/databases/merops/current_release/pepunit.lib -o pepunit.lib 8.3 Mebs See installation and instructions on the Github page perl mebs.pl -input /home/kvilleneuve/Shotgun_Project/Melanie_Shotgun/prodigal -type genomic -comp &gt; rock_MAGS.tsv python3 mebs_vis.py rock_assembly.tsv -o /home/kvilleneuve/Shotgun_Project/Melanie_Shotgun/rock_assembly_MEBS -im_format pdf -f pdf "],["phylogenetic-tree.html", "9 Phylogenetic tree 9.1 Downloading reference genomes 9.2 GToTree", " 9 Phylogenetic tree 9.1 Downloading reference genomes Download the latest NCBI assembly_summary_genbank.txt file using wget. Note that the link might change over time. wget https://ftp.ncbi.nih.gov/genomes/genbank/assembly_summary_genbank.txt For each phylum, order, class, family of interest, search in the NCBI database and select 5 individuals with the most complete genome. Copy/paste the name and the assembly number in an excel document (seperate columns - Name and Assembly_GCA). From that excel document, copy the list of assembly number including the column name (Assembly_GCA) in a vi document called GCA.txt. Search for those identifiers in the assembly summary genbank file with the following command and export the columns (1,7,8,20*) we are interested in to a tabular file called todownload.tab. for n in `cat GCA.txt`; do grep $n assembly_summary_genbank.txt | cut -f 1,7,8,16,20 &gt;&gt; todownload.tab ; done Column 1: “assembly_accession” - Column 7: “species_taxid” - Column 8: “organism_name” - Column 16: “asm_name” - Column 20: “ftp_path” Run the Rscript getFTPlink.r which generates two files 1. ftp.links.txt : contains the ftp link to the genomic.fna.gz file for each genomes of interest. Download the files using wget and unzip them. 2. name.txt : contains two column, first is the name of the fasta file followed by organsim name and asm name. This file will be used to change the branch names for the tree Rscript /home/SCRIPT/getFTPlink.r wget -i ftp.links.txt gunzip *.gz Create a new directory and move the ref files there and run CheckM on the downloaded genomes. Discard the genomes that are not completed. conda activate checkm checkm lineage_wf -x fna ref_genomes checkm -f output_table.txt -t 48 Move your bins and reference genomes in a folder together. Add the name of the bin to the beginning of every file and change the file type to .fna. for i in *.fa ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done 9.1.1 Bash script #!/bin/bash echo &quot;Getting data from assembly_summary_genbank.txt based on GCA.txt&quot; for n in `cat GCA.txt`; do grep $n assembly_summary_genbank.txt | cut -f 1,7,8,16,20 &gt;&gt; todownload.tab ; done echo &quot;Getting ftp link to download genomes&quot; Rscript /home/SCRIPT/getFTPlink.r echo &quot;Downloading and unzipping genomes&quot; wget -i ftp.links.txt gunzip *.gz 9.2 GToTree Github Move the file name.txt into the folder containing your genomes. Add to this file the name of your bins. Create a txt file called fasta_files.txt containing the name of all the fasta files you have. ls *.fna &gt; fasta_files.txt conda activate gtotree GToTree -f fasta_files.txt -H Bacteria -j 4 -o out -G 0 Upload the tree to itol "],["using-nohup-and-other-tips-and-tricks.html", "10 Using nohup and other tips and tricks 10.1 Bash scripting using nohup", " 10 Using nohup and other tips and tricks 10.1 Bash scripting using nohup Fist step is to write your script into a text editor (vi or nano). I suggest using vi as it is a very powerful text editor with plenty of useful shortcuts. You can find more information on vi here. To create a new file the format of the command is as follow : vi script.sh First we call the program vi followed by the name of file to create. The extension of the file needs to reflect the type of file you want to create (i.e. for a python script the extension would be .py, a bash script would be .sh, and a simple text file would be .txt. Since the commands we are writing are for executed by bash we need to use the extension .sh. In the new opened window write your script. To start typing with vi press the i touch. Once you are done exit vi by pressing esc then : followed by w and q. If you want to exit without saving your changes press q and ! instead. Make your script executable. If you look at the file using ll the script should now be green and followed by *. chmod +x script.sh Run using nohup nohup ./script.sh &amp; After pressing nohup the prompt will print your process ID in the following format [1] xxxxxxxx and print the following : nohup: ignoring input and appending output to 'nohup.out' Press enter again to return to the prompt. Your process is completed when you get the following [1]+ Done nohup ./script.sh. If nohup exits there was an error running the script. To view the output and the find out the error, view the report using less nohup.out. Exit less with q View jobs status jobs Once nohup is done running your script it will write Done "],["other-tips-and-tricks.html", "11 Other tips and tricks 11.1 Moving files using SCP 11.2 Renaiming files 11.3 Extract scaffhold based on nane and coordinates using samtools", " 11 Other tips and tricks To move or copy many files listed in a text files xargs -a file_list.txt mv -t /path/to/directory 11.1 Moving files using SCP To move file from your computer to the server, from your local terminal scp filename.txt servername@server.bio.uqam.ca:/home/user/pathway/todirectory To move file from your the server, to your computer scp kvilleneuve@titan.bio.uqam.ca:/home/kvilleneuve/filename.txt . 11.2 Renaiming files Replace the pattern in brackets for the section of words you want to remove from the name of your file. for i in *.fastq; do mv $i &quot;$(echo $i | sed s/&quot;_R1.fastq.interleave.fastq.trim.fastq.gz.&quot;/./)&quot;; done 11.3 Extract scaffhold based on nane and coordinates using samtools Copy the scaffhold name followed by the coordinates in a vi file. Example : contig-115_1168:1-955 Extract the sequence identified in the vi file into a new file using samtools for i in `cat scaff`; do samtools faidx 2000kb.fa $i &gt;&gt;16S_2Kb.fna; done "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
