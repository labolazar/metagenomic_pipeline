---
title: "Protocol for metagenomic analyses"
author: "Karine Villeneuve"
date: "`r Sys.Date()`"
output: rmdformats::robobook
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(eval = FALSE) 
``` 

Some program may take some time to process, we therefor recommend running the command using a shell script with **nohup**. For example on how to use, see section `Nohup`. Furthermore, two ways of running most command are described; (**1**) for one or two samples and (**2**) for multiple samples using Bash For Loop. 

Conda and all the required environment have all been previously installed by Karine Villeneuve.

# Pre-Assembly

1. Unzip 

Copy all your raw `fastq.gz` files into a folder called `02_preprocess`. Move into the folder and unzip the files.  
```{bash, eval=FALSE}
gunzip *.gz
```

2. Interleave 

We are using a modified version of the script <font color='green'>interleave.py</font> from this [GitHub gist](https://gist.github.com/ngcrawford/2232505) to interleave the forward and reverse read. Create bash script called `run_interleave.sh` with the following commands and execute the script using nohup. 

```{bash}
#!/bin/bash
for R1 in *R1*.fastq;
do 
  python3 /home/SCRIPT/interleave.py $R1 "${R1/R1/R2}"  > $R1.interleave.fastq ; 
done 
```

**Output** : For each given **sample** (not file) as input it generates a new file ending in `.interleave.fastq` will be generated. Create a new directory called `interleave` and move all the `.interleave.fastq` in this new directory. Move into this new directory or downstream analysis. 

View the top of the new interleaved files to make sure your reads alternate between R1 and R2. Replace sample-name with the name of the sample you want to verify. For sample sequenced on the NovaSeq replace `@M` for `@A`. 
```{bash, eval=FALSE}
grep @M sample-name.interleave.fastq | head
```

3. Trim 

[Sickle](https://github.com/najoshi/sickle) is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3’-end of reads and also determines when the quality is sufficiently high enough to trim the 5’-end of reads. Create bash script called `run_sickle.sh` with the following commands and execute the script using nohup. 

```{bash}
#!/bin/bash
for i in *.interleave.fastq
  do sickle pe -c $i -t sanger -m $i.trim.fastq -s $i.singles.fastq
done
```
*sickle pe (paired end) -c (inputfile) -t sanger (from illumina) -m (outputfilename) -s (exclutedreadsfilename)*

**Output** : For each given file it generates two files (one ending in `.interleave.fastq.trim.fastq` and the other ending in `.interleave.fastq.singles.fastq`). For downstream processing we only need the `.interleave.fastq.trim.fastq` and therefore you can move all the `.single` into a new directory called `single`.

4. Quality check

[FastQc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) provides a modular set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.

Create a new folder directory called `fastqc` which is where the HTLM output of FastQc will be saved. Create bash script called `run_fastqc.sh` with the following commands and execute the script using nohup. 

```{bash}
#!/bin/bash 
fastqc *.interleave.fastq.trim.fastq --outdir=fastqc
``` 

**Output** : For each given file it generates an HTML file. The quality of the samples can be assessed by transferring the HTML files to your local computer. To determine if trimming caused any issue you can also run FastQc on the unzipped raw file and compare the results with the interleaved-trimmed file. 

5. Transfer to Fasta 

[Seqtk](https://github.com/lh3/seqtk) is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. 

Move all the `.interleave.fastq.trim.fastq` to a new folder called `03_trim_interleave` 

```{bash, eval=FALSE}
#!/bin/bash
gzip *.fastq
for i in *.gz ; 
  do seqtk seq -a $i > $i.fasta ; 
done 
```

***

# Assembly 

## Assemblers 

For the moment, there is no way of knowing which assembly program is best suited for your sample. Therefor, I recommend trying different assemblers and looking at (1) the total number of contigs, (2) the number of contigs > 2000 bp and (3) the number of contigs > 1000 bp. Because the assembly process is quite long I recommend always using nohup. Most assembler will produce a file called **contig.fa** or **contigs.fa** which is the file we will be using to generate bins. 

[About co-assembly](https://angus.readthedocs.io/en/2019/recovering-rep-genomes-from-mgs.html)

[Cross-Assembly pipeline](https://linsalrob.github.io/ComputationalGenomicsManual/CrossAssembly/)

[Further reading](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-017-3918-9)

From the same directory containing `01_Pre-Assembly`, create a new directory called `02_assembly`. In this new directory create a directory for every assembly program (`IDBA`, `Megahit`, `Metaspades`)
```{bash, eval=FALSE}
#!/bin/bash
mkdir 02_assembly
cd 02_assembly
mkdir IDBA
mkdir Megahit
mkdir Metaspades 
cd ..
``` 

### IDBA 
[Github](https://github.com/loneknightpy/idba). The output is a directory ending in `assembly` for each sample. In this directory you will find the contig file.  

Run IDBA using nohup 

For DNA
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fasta
  do idba_ud -l -r $i -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/IDBA/$i --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 40 
done
```
For RNA 
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fasta
  do idba_tran -l -r $i -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/IDBA/$i --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 40 
done
```


### Megahit 
[Github](https://github.com/voutcn/megahit)
Run Megahit using nohup 
```{bash, eval=FALSE}
#!/bin/bash
for i in *.fasta
  do megahit --12 $i --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 40 -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/Megahit/$i --out-prefix megahit_$i  
done
``` 

### Metaspades 
[Github](https://github.com/ablab/spades). We are using the **fastq** files. 

To run Metaspades you will need to add the location of the program to your path using the following command
```{bash}
export PATH=/home/SCRIPT/SPAdes-3.15.5-Linux/bin:$PATH
```
Run Metaspades using nohup 
```{bash}
#!/bin/bash
for i in *.fastq 
  do metaspades.py --12 $i -o /home/kvilleneuve/Shotgun_Project/saumure_grotte/02_assembly/Metaspades/$i -t 40 
done
``` 


## Post assembly-stat 
To determine which assembly method gave you the best results you need to look at the output of every sample individually. For every sample, the assembler will generate a folder containing a file called contig.fa. To know how many contigs were generated we use this command which counts the number of ">" (every contig start with this symbol): 

```{bash}
grep -c ">" contig.fa 
``` 

We also want to know how many contig have more than 2000 base pairs (2KB) and 1000 base pairs (1KB). For each of your sample, run the following script which will output two files :
- 2kb.fasta (the number of contigs with length greater than 2000 basepair)
- 1kb.fasta (the number of contigs with length greater than 1000 basepair)
```{bash}
bash /home/SCRIPT/countBP.sh
``` 

```{bash}
#!/bin/bash
perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print "$s\n$fa{$s}\n" if(length($fa{$s})>1000) }}' contig.fa > 1kb.fa
perl -lne 'if(/^(>.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print "$s\n$fa{$s}\n" if(length($fa{$s})>2000) }}' contig.fa > 2kb.fa

grep -c ">" 1kb.fa
grep -c ">" 2kb.fa
``` 

# Binning 

In metagenomics, binning is the process of grouping reads or contigs and assigning them to individual genome known as Metagenome Assembled Genome (MAG). Coverage-based binning approaches will require you to map reads to assembled contigs. 

[Comparision of different binning tools](https://bitbucket.org/berkeleylab/metabat/wiki/Home)

## Mapping 

Mapping allows you to get a rough count of the total number of reads mapping to each contig, also referred to as the depth file or read coverage. This information is required for most binning algorithm. Raw reads from each sample are mapped to the contigs (assembled reads).The idea being that if two contigs come from the same genome in your sample, so the same organism, then they would have been sequenced roughly to the same depth, they would have a similar coverage and they go together. It gets even better if you have multiple samples that vary a bit and you sequence all of them. You do your assembly on one of the samples, then you can take the reads from the other samples, map to that genome (metagenomic assembly) and you can use that as additional information. 

Start by creating a new directory with all your assembled contigs and your interleave.trim.fastq. Different tools exists for mapping reads to genomic sequences. Here we are using `Scons wrapper`. 

### Scons Wrapper (genome mapping)
[Github](https://github.com/imrambo/genome_mapping)

This wrapper maps **FASTQ** reads against an assembly (e.g. genome) in **FASTA** format using BWA-MEM. This wrapper does not produce huge intermediate files (e.g. unfiltered SAM). 

For each sample, create a folder and copy into this folder these two files : the **long contig.fa** (1000 or 2000 bp) and the **trim and interleaved fastq** (fastq after sickle)

1. Go to your home directory and clone the git repository 

```{bash, highlight=TRUE, eval=FALSE}
git clone https://github.com/imrambo/genome_mapping.git
```

2. Go to the new directory created `genome_mapping` and activate the scons_maps conda environment
```{bash, highlight=TRUE, eval=FALSE}
conda activate scons_map
```

3. Run a dry run to ensure everything runs smoothly (you must run this from the `genome_mapping` directory)
```{bash, highlight=TRUE, eval=FALSE}
scons --dry-run --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

4. Run the script from the `genome_mapping` directory. 

--fastq_dir=directory where the interleave_trim_fastq and assembled_long_contig_fasta files are.

--assembly=path to the assembled_long_contig_fasta files. Must include the name of the file at the end. 

--sampleids=the name of the interleave_trim_fastq. 

```{bash, highlight=TRUE, eval=FALSE}
scons --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

In the specified output directory you will find a `.sorted.bam` which contains the depth information required for mmgenome and metabat. 

<font color='red'>Currently looking for a way to loop this. Tried specifying only the input folder and the sampleids as proposed in the README but it does not work. `sampleids` **must** be the exact name as the `fastq` or `fastq.gz` file.</font>

## Depth file

The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use. We use the script `jgi_summarize_bam_contig_depths`.  

Move all the `sorted.bam` files into a new folder called `Sorted_Bam` and from this folder use nohup to run the script `jgi_summarize_bam_contig_depths`. 

```{bash}
#!/bin/bash
for i in *.sorted.bam
  do /home/SCRIPT/jgi_summarize_bam_contig_depths --outputDepth $i.depth.txt --pairedContigs $i.paired.txt $i
done 
```

## Create bins  

### MetaBAT2 

[Github](https://bitbucket.org/berkeleylab/metabat/src/master/) / [Article](https://peerj.com/articles/1165/)
Efficient tool for accurately reconstructing single genomes from complex microbial communities

MetaBAT2 requires that your python environment be activate (base). If required, first deactivate `scons_map` and then activate conda
```{bash}
conda deactivate
conda activate
```

In order to be able to loop this for all my samples, I renamed each depth file in the following format : `sample1.fa.depth.txt`. The output is a folder called `bins_dir` containing all the bins created. I recommend using nohup as the binning process can be very long. 

**copy all your 1kb or 2 kb fasta into the folder containing the depth file. 

**Multiple samples**
```{bash}
#!/bin/bash
for i in *.fa
  do metabat2 -i $i -a $i.depth.txt -o bins -t 0 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
done 
```

`minCVsum` : assigning number of tetranucleotide frequency graphs, don’t grab negative numbers 
`-m` : min size of contig to be considered for binning

# Bin quality  {.tabset}
## Checkm
[Github](https://github.com/Ecogenomics/CheckM/wiki)

**You have to go back one folder in the terminal as checkm will run on all the files in the folder you give it as input. Checkm will automatically create a folder called checkm in the specified directory, therefor if you must run checkm again make sure to delete the newly created checkm folder, otherwise checkm will give you an error message.** 

You  need to specify the extension of your file for it to work. For example, for file finishing is `.fa` the command will be `checkm lineage_wf -x fa`... 

If checkm is already installed on your system simply activate the environment
```{bash, highlight=TRUE, eval=FALSE}
conda activate checkm
```
Run checkm using nohup 
```{bash, highlight=TRUE, eval=FALSE}
checkm lineage_wf -x fa 2kbp_bins/ 2kbp_bins/checkm -f 2kbp_bins/output.txt -t 48 
```

checkm lineage_wf -x fa 1500bp_bins 1500bp_bins/checkm -f 1500bp_bins/output.txt -t 48 --noAdd

a. Open the `output.txt` document with excel to verify the **completeness** and **contamination** of your bins. 
**Standard : Completeness > 50 % and Contamination < 10 %**

b. Remove all the spaces with `control` + `H`

c. Filter the columns by Completeness, and separate the ones < 50 % by adding a line in excel 

d. Filter by Contamination, and highlight all the ones > 10 % - These are the bins you want to clean

## Installing Checkm 
Install Checkm using the [Installation through Conda](https://github.com/Ecogenomics/CheckM/wiki/Installation#how-to-install-checkm) steps. 
After installation is complete run the following to inform where the checkm databases are installed*:
```{bash, highlight=TRUE, eval=FALSE}
checkm data setRoot /usr/local/lib/checkm
```

*I downloaded the [checkm databases](https://data.ace.uq.edu.au/public/CheckM_databases/) and moved them to /usr/local/lib/checkm. 
I decompressed the file using `sudo tar -xf checkm_data_2015_01_16.tar.gz`. 
I changed the File Ownership to root `sudo chown -R root /usr/local/lib/checkm` and Group Ownership to me `sudo chgrp kvilleneuve /usr/local/lib/checkm`. 
I ran the following to inform CheckM of where the files have been placed: `checkm data setRoot /usr/local/lib/checkm`


# Bin cleaning 

## Vizbin 

# Taxonomy 

## GTDBTK
[Github](https://github.com/Ecogenomics/GTDBTk)

Activate the GTDBTK environment
```{bash, highlight=TRUE, eval=FALSE}
conda activate gtdbtk-2.1.1
``` 

I located the folder with the untar GTDBTK data (GTDBTk_data/release214) and I added the path to this file to my ~/.profile (using vi)
```{bash, highlight=TRUE, eval=FALSE}
export GTDBTK_DATA_PATH=/home/genomics/release214
```

In the folder with all your clean and completed genomes run this command with nohup

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
gtdbtk classify_wf --cpus 20 --genome_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp --out_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp/gtdbk_output -x fa
```


Once it is done running, you can open the folder called `gtdbk_output` and copy the folder `gtdbtk.bac120.summary.tsv` to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. 

## BAsic Rapid Ribosomal RNA Predictor (Barrnap) 
[Github](https://github.com/tseemann/barrnap)

Barrnap predicts the location of ribosomal RNA genes in genomes. It supports bacteria (5S,23S,16S), archaea (5S,5.8S,23S,16S), metazoan mitochondria (12S,16S) and eukaryotes (5S,5.8S,28S,18S). You can run barrnap on both the assembled contigs (community) and MAGs. 

Add the name of the sample at the beginning of every contig and change the file type to `.fna`. Then for each of your file, change the space between the name and the scaffhold number to an underscore

```{bash, highlight=TRUE, eval=FALSE}
for i in *.fa ; do  perl -lne 'if(/^>(\S+)/){ print ">$ARGV $1"} else{ print }' $i > $i.fna ; done
sed -i 's/ /_/g' *.fna
```
Run barrnap
```{bash, highlight=TRUE, eval=FALSE}
barrnap 1kb.fasta.fna > barrnap_hits.txt --threads 20
```
One very useful tool or parsing GFF files is called BEDtool (to install:sudo apt install bedtools). There are many different utilities in bedtools. Here we will want to use the "getfasta" option, which will allow us to supply the fasta file and the barrnap GFF file to obtain the rRNA sequences. Note that the GFF file has the coordinates of where the rRNA genes are encoded, so between the GFF file and the .FNA file we have all the information we need. 
```{bash, highlight=TRUE, eval=FALSE}
bedtools getfasta -fi 1kb.fasta.fna -bed barrnap_hits.gff -fo out.rRNA.fasta
```

Use NCBI-BLAST to classify the rRNA sequences identified by barrnap. 

# Metabolic pathway 

Both DRAM and Metabolic are installed only for the user `genomics`
- User : ssh genomics@titan.bio.uqam
- Password : genomics

Use `sudo cp` to copy the samples to annotate into your directory under the user genomics. 

## DRAM
[Github page](https://github.com/WrightonLabCSU/DRAM)

Note : DRAM requires the full path (home/genomics/...) and won't accept path starting from the home directory (~).
```{bash, highlight=TRUE, eval=FALSE}
conda activate DRAM
``` 
```{bash, highlight=TRUE, eval=FALSE}
DRAM.py annotate -i '/home/genomics/yourname/samples/*.fa' -o /home/genomics/yourname/samples/annotation --threads 40 
``` 
Once annotation is done, the following command will summarize all the results from the folder `annotate` 
```{bash, highlight=TRUE, eval=FALSE}
DRAM.py distill -i annotation/annotations.tsv -o summaries --trna_path annotation/trnas.tsv --rrna_path annotation/rrnas.tsv
``` 

### Installing

Because FTP connections weren't allowed on the server, I changed the link to download the different databases from FTP to HTTPS. The file I modified is `/home/kvilleneuve/anaconda/envs/DRAM/lib/python3.10/site-packages/mag_annotator/database_processing.py`. 

## Metabolic {.tabset}

**Prodigal** 
[Github](https://github.com/hyattpd/Prodigal)

We use prodigal to translate nucleic acid sequences to the corresponding peptide sequences.By default prodigal takes as input fasta files with the `.fna` extension. Use perl to add the name of the sample to the beginning of every file and change the file type to `.fna`. 
```{bash, highlight=TRUE, eval=FALSE}
for i in *.fa ; do  perl -lne 'if(/^>(\S+)/){ print ">$ARGV $1"} else{ print }' $i > $i.fna ; done
``` 
Replace space between the name of you bins and the scaffhold to underscores 
```{bash, highlight=TRUE, eval=FALSE}
for i in *.fna ; do sed -i 's/ /_/g' $i ; done 
```
Run `prodigal` to convert your bins to amino acid (faa) sequence
```{bash, highlight=TRUE, eval=FALSE}
for i in *.fna ; do prodigal -i $i -o output.txt -a $i.faa ; done
```
Remove all the characters after the first space in the header 
```{bash, highlight=TRUE, eval=FALSE}
for i in *.faa; do sed -i 's/\s.*$//' $i; done 
```

**Metabolic**

I am currently having issues again with running and installing metabolic....

[Github](https://github.com/AnantharamanLab/METABOLIC)
```{bash, highlight=TRUE, eval=FALSE}
conda activate metabolic
``` 

Change the path to the directory containing the genome amino acid files ending with `.faa` Run using nohup 
```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
perl /home/genomics/METABOLIC/METABOLIC-G.pl -in /path/to/faa/ -o output/ 
``` 

### Installing

To install metabolic I copied the file `metabolic_environment.txt` file from the [issue #27](https://github.com/AnantharamanLab/METABOLIC/issues/27#issuecomment-777138057) on the server and renamed it `metabolic_environment.yml`

I created the environment using `conda env create -f metabolic_environment.yml` and then followed all the other step described in the [Github wiki](https://github.com/AnantharamanLab/METABOLIC/wiki)

There also seemed to be issues with the run_to_setup.bash. I solved this problem by changing these commands in the script : 
```{bash, highlight=TRUE, eval=FALSE}
curl --silent ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz  -O ko_list.gz
curl --silent ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz  -O profiles.tar.gz
curl --silent ftp://ftp.ebi.ac.uk/pub/databases/merops/current_release/pepunit.lib -O pepunit.lib
``` 
for these commands : 
```{bash, highlight=TRUE, eval=FALSE}
curl https://www.genome.jp/ftp/db/kofam/ko_list.gz  -output ko_list.gz
curl https://www.genome.jp/ftp/db/kofam/profiles.tar.gz  -output profiles.tar.gz
curl https://ftp.ebi.ac.uk/pub/databases/merops/current_release/pepunit.lib -o pepunit.lib
``` 

## Mebs 

See installation and instructions on the [Github page](https://github.com/valdeanda/mebs)

```{bash, highlight=TRUE, eval=FALSE}
perl mebs.pl -input /home/kvilleneuve/Shotgun_Project/Melanie_Shotgun/prodigal -type genomic -comp > rock_MAGS.tsv
```

```{bash, highlight=TRUE, eval=FALSE}
python3 mebs_vis.py rock_assembly.tsv -o /home/kvilleneuve/Shotgun_Project/Melanie_Shotgun/rock_assembly_MEBS -im_format pdf -f pdf
``` 

# Phylogenetic tree 

## Downloading reference genomes 

Download the latest NCBI `assembly_summary_genbank.txt` file using wget. Note that the link might change over time. 
```{bash, highlight=TRUE, eval=FALSE}
wget https://ftp.ncbi.nih.gov/genomes/genbank/assembly_summary_genbank.txt
``` 

For each phylum, order, class, family of interest, search in the [NCBI database](https://www.ncbi.nlm.nih.gov/genome/browse#!/overview/) and select 5 individuals with the most complete genome. Copy/paste the name and the assembly number in an excel document (seperate columns - Name and Assembly_GCA). From that excel document, copy the list of assembly number **including the column name (Assembly_GCA)** in a vi document called `GCA.txt`. Search for those identifiers in the assembly summary genbank file with the following command and export the columns (1,7,8,20*) we are interested in to a tabular file called `todownload.tab`. 
```{bash, highlight=TRUE, eval=FALSE}
for n in `cat GCA.txt`; do grep $n assembly_summary_genbank.txt  | cut -f 1,7,8,16,20 >> todownload.tab ; done
```  
* *Column  1: "assembly_accession" - Column  7: "species_taxid" - Column  8: "organism_name" - Column 16: "asm_name" - Column 20: "ftp_path"* 

Run the Rscript getFTPlink.r which generates two files 
1. `ftp.links.txt` : contains the ftp link to the genomic.fna.gz file for each genomes of interest. Download the files using wget and unzip them. 
2. `name.txt` : contains two column, first is the name of the fasta file followed by organsim name and asm name. This file will be used to change the branch names for the tree 

```{bash, highlight=TRUE, eval=FALSE}
Rscript /home/SCRIPT/getFTPlink.r 
wget -i ftp.links.txt
gunzip *.gz
``` 
Create a new directory and move the ref files there and run CheckM on the downloaded genomes. Discard the genomes that are not completed.
```{bash, highlight=TRUE, eval=FALSE}
conda activate checkm
checkm lineage_wf -x fna ref_genomes checkm -f output_table.txt -t 48
``` 

Move your bins and reference genomes in a folder together. Add the name of the bin to the beginning of every file and change the file type to .fna. 
```{bash, highlight=TRUE, eval=FALSE}
for i in *.fa ; do  perl -lne 'if(/^>(\S+)/){ print ">$ARGV $1"} else{ print }' $i > $i.fna ; done
``` 

### Bash script 
```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
echo "Getting data from assembly_summary_genbank.txt based on GCA.txt" 
for n in `cat GCA.txt`; do grep $n assembly_summary_genbank.txt  | cut -f 1,7,8,16,20 >> todownload.tab ; done
echo "Getting ftp link to download genomes" 
Rscript /home/SCRIPT/getFTPlink.r 
echo "Downloading and unzipping genomes"
wget -i ftp.links.txt
gunzip *.gz
```  

## GToTree
[Github](https://github.com/AstrobioMike/GToTree)

Move the file `name.txt` into the folder containing your genomes. Add to this file the name of your bins. 

Create a txt file called `fasta_files.txt` containing the name of all the fasta files you have. 

```{bash, highlight=TRUE, eval=FALSE}
ls *.fna > fasta_files.txt
conda activate gtotree
GToTree -f fasta_files.txt -H Bacteria -j 4 -o out -G 0 
``` 

Upload the tree to itol
