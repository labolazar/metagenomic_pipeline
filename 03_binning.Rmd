# Binning 

In metagenomics, binning is the process of grouping reads or contigs and assigning them to individual genome known as Metagenome Assembled Genome (MAG). Coverage-based binning approaches will require you to map reads to assembled contigs. 

[Comparision of different binning tools](https://bitbucket.org/berkeleylab/metabat/wiki/Home)

**1. Mapping**

Mapping allows you to get a rough count of the total number of reads mapping to each contig, also referred to as the depth file or read coverage. This information is required for most binning algorithm. Raw reads from each sample are mapped to the contigs (assembled reads).The idea being that if two contigs come from the same genome in your sample, so the same organism, then they would have been sequenced roughly to the same depth, they would have a similar coverage and they go together. It gets even better if you have multiple samples that vary a bit and you sequence all of them. You do your assembly on one of the samples, then you can take the reads from the other samples, map to that genome (metagenomic assembly) and you can use that as additional information. 

Different tools exists for mapping reads to genomic sequences. Here we are using [Scons Wrapper ](https://github.com/imrambo/genome_mapping) which maps FASTQ reads against an assembly (e.g. genome) in FASTA format using BWA-MEM. This wrapper does not produce huge intermediate files (e.g. unfiltered SAM). 

<font color='red'>I am currently looking for a way to loop this. Tried specifying only the input folder and the sampleids as proposed in the README but it does not work. Therefore these steps must be repeated for every sample. .</font>

For each sample, create a new folder in the directory `05_binning` and inside each folder copy the filtered assembled contigs and the interleaved-trimed FASTQ for that sample.

1. Go to your home directory and clone the git repository 

```{bash, highlight=TRUE, eval=FALSE}
git clone https://github.com/imrambo/genome_mapping.git
```

2. Go to the new directory created `genome_mapping` and activate the scons_maps conda environment
```{bash, highlight=TRUE, eval=FALSE}
conda activate scons_map
```

`sampleids` **must** be the exact name as the `fastq` or `fastq.gz` file

3. Run a dry run to ensure everything runs smoothly (you must run this from the `genome_mapping` directory)
```{bash, highlight=TRUE, eval=FALSE}
scons --dry-run --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

4. Run the script from the `genome_mapping` directory. 

--fastq_dir=directory where the interleave_trim_fastq and assembled_long_contig_fasta files are.

--assembly=path to the assembled_long_contig_fasta files. Must include the name of the file at the end. 

--sampleids=the name of the interleave_trim_fastq. 

```{bash, highlight=TRUE, eval=FALSE}
scons --fastq_dir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/ --assembly=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/V01_2_1000bp.fa --outdir=/home/kvilleneuve/Metagenomic_analyses/mapping/V01_2/output --sampleids=V01_2_S7_L001_R1_001.fastq.interleave.fastq.trim.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/kvilleneuve/tmp --logfile=mapping.log
```

In the specified output directory you will find a `.sorted.bam` which contains the depth information required for mmgenome and metabat. 

**2. Generate depth file**

The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use. We use the script `jgi_summarize_bam_contig_depths`.  

Move all the `sorted.bam` files into a new folder called `Sorted_Bam` and from this folder use nohup to run the script `jgi_summarize_bam_contig_depths`. 

```{bash}
#!/bin/bash
for i in *.sorted.bam
  do /home/SCRIPT/jgi_summarize_bam_contig_depths --outputDepth $i.depth.txt --pairedContigs $i.paired.txt $i
done 
```

**3. Create bins**

### MetaBAT2 

[Github](https://bitbucket.org/berkeleylab/metabat/src/master/) / [Article](https://peerj.com/articles/1165/)
Efficient tool for accurately reconstructing single genomes from complex microbial communities

MetaBAT2 requires that your python environment be activate (base). If required, first deactivate `scons_map` and then activate conda
```{bash}
conda deactivate
conda activate
```

In order to be able to loop this for all my samples, I renamed each depth file in the following format : `sample1.fa.depth.txt`. The output is a folder called `bins_dir` containing all the bins created. I recommend using nohup as the binning process can be very long. 

**copy all your 1kb or 2 kb fasta into the folder containing the depth file. 

**Multiple samples**
```{bash}
#!/bin/bash
for i in *.fa
  do metabat2 -i $i -a $i.depth.txt -o bins -t 0 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
done 
```

`minCVsum` : assigning number of tetranucleotide frequency graphs, donâ€™t grab negative numbers 
`-m` : min size of contig to be considered for binning

**4. Check quality**
## Checkm
[Github](https://github.com/Ecogenomics/CheckM/wiki)

**You have to go back one folder in the terminal as checkm will run on all the files in the folder you give it as input. Checkm will automatically create a folder called checkm in the specified directory, therefor if you must run checkm again make sure to delete the newly created checkm folder, otherwise checkm will give you an error message.** 

You  need to specify the extension of your file for it to work. For example, for file finishing is `.fa` the command will be `checkm lineage_wf -x fa`... 

If checkm is already installed on your system simply activate the environment
```{bash, highlight=TRUE, eval=FALSE}
conda activate checkm
```
Run checkm using nohup 
```{bash, highlight=TRUE, eval=FALSE}
checkm lineage_wf -x fa 2kbp_bins/ 2kbp_bins/checkm -f 2kbp_bins/output.txt -t 48 
```

checkm lineage_wf -x fa 1500bp_bins 1500bp_bins/checkm -f 1500bp_bins/output.txt -t 48 --noAdd

a. Open the `output.txt` document with excel to verify the **completeness** and **contamination** of your bins. 
**Standard : Completeness > 50 % and Contamination < 10 %**

b. Remove all the spaces with `control` + `H`

c. Filter the columns by Completeness, and separate the ones < 50 % by adding a line in excel 

d. Filter by Contamination, and highlight all the ones > 10 % - These are the bins you want to clean

## Installing Checkm 
Install Checkm using the [Installation through Conda](https://github.com/Ecogenomics/CheckM/wiki/Installation#how-to-install-checkm) steps. 
After installation is complete run the following to inform where the checkm databases are installed*:
```{bash, highlight=TRUE, eval=FALSE}
checkm data setRoot /usr/local/lib/checkm
```

*I downloaded the [checkm databases](https://data.ace.uq.edu.au/public/CheckM_databases/) and moved them to /usr/local/lib/checkm. 
I decompressed the file using `sudo tar -xf checkm_data_2015_01_16.tar.gz`. 
I changed the File Ownership to root `sudo chown -R root /usr/local/lib/checkm` and Group Ownership to me `sudo chgrp kvilleneuve /usr/local/lib/checkm`. 
I ran the following to inform CheckM of where the files have been placed: `checkm data setRoot /usr/local/lib/checkm`

**5. Metagenome assembled genome statistics**

```{bash}
#!/bin/bash
for i in *.fa
  do  python /home/genomics/quast-5.2.0/quast.py $i -o /home/genomics/karine/stleonard/E4-2/quast_out/$i --threads 48 --no-check --no-plots --no-html --no-icarus
done
```

```{bash, eval=FALSE}
for folder in *; do
    (cd "$folder" && report.txt)  
    mv "$folder/report.txt" "${folder}_report.txt"
done
```

**6. Taxonomy**

[GTDBTK](https://github.com/Ecogenomics/GTDBTk)

Activate the GTDBTK environment
```{bash, highlight=TRUE, eval=FALSE}
conda activate gtdbtk-2.1.1
``` 

I located the folder with the untar GTDBTK data (GTDBTk_data/release214) and I added the path to this file to my ~/.profile (using vi)
```{bash, highlight=TRUE, eval=FALSE}
export GTDBTK_DATA_PATH=/home/genomics/release214
```

In the folder with all your clean and completed genomes run this command with nohup

```{bash, highlight=TRUE, eval=FALSE}
#!/bin/bash
gtdbtk classify_wf --cpus 20 --genome_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp --out_dir /home/kvilleneuve/Shotgun_Project/saumure/05_binning/complete_bins_1500bp/gtdbk_output -x fa
```


Once it is done running, you can open the folder called `gtdbk_output` and copy the folder `gtdbtk.bac120.summary.tsv` to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. 
